<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Gaoustcer blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Gaoustcer blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Gaoustcer blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Haihan Gao">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Gaoustcer blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Gaoustcer blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Approximation-Algorithm-Sparest-Cut" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/19/Approximation-Algorithm-Sparest-Cut/" class="article-date">
  <time class="dt-published" datetime="2023-02-19T03:10:58.000Z" itemprop="datePublished">2023-02-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/19/Approximation-Algorithm-Sparest-Cut/">Approximation Algorithm Sparest Cut</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="approximation-algorithmsparsest-cut">Approximation Algorithm(Sparsest Cut)</h1>
<h2 id="metric-embedding">Metric embedding</h2>
<h3 id="embedding-metric">Embedding metric</h3>
<figure>
<img src="https://s2.loli.net/2023/02/03/Yb3fq65CekyjxFS.png" alt="image-20230203165751416" /><figcaption aria-hidden="true">image-20230203165751416</figcaption>
</figure>
<p>d是V上的一种距离度量</p>
<blockquote>
<p>注意是先确定d在找到合适的f</p>
</blockquote>
<h3 id="metric-embedding-in-a-graph">Metric embedding in a graph</h3>
<p>定义图G上cut S的indictor function <span class="math inline">\(\chi_{\delta(S)}(u,v)\)</span> <span class="math display">\[
\chi_{\delta(S)}(u,v) =\left\{
\begin{aligned}
&amp;1 ,u/v \ exactly\ one \ in\ S\\
&amp;0,else
\end{aligned}
\right.
\]</span></p>
<h3 id="metric-embedding-with-distortion-alpha">Metric Embedding with distortion <span class="math inline">\(\alpha\)</span></h3>
<p>有时候不能找到准确的函数f使得满足定义1，因此提出distortion形式，<span class="math inline">\(\alpha\)</span>决定了upper bound和lower bound之间的Gap</p>
<figure>
<img src="https://s2.loli.net/2023/02/03/IpfRycHsMBmSea3.png" alt="image-20230203172331516" /><figcaption aria-hidden="true">image-20230203172331516</figcaption>
</figure>
<blockquote>
<p>定理：任何度量空间可以嵌入到<span class="math inline">\(l_1\)</span>空间中，distortion是<span class="math inline">\(O(\log n)\)</span>量级，并且计算<span class="math inline">\(f:V\to \R^m\)</span>是多项式时间的函数只要<span class="math inline">\(m= O(\log^2n)\)</span></p>
<figure>
<img src="https://s2.loli.net/2023/02/19/1GNbCatcX49ISPw.png" alt="image-20230203172454896" /><figcaption aria-hidden="true">image-20230203172454896</figcaption>
</figure>
<p>换成人话，对于<span class="math inline">\(G =(V,E)|V|=n\)</span>，可以找一个distortion <span class="math inline">\(\alpha = o(\log n)\)</span>的<span class="math inline">\(l_1\)</span> embedding function <span class="math inline">\(f\)</span>，并且<span class="math inline">\(m = O(\log ^2n)\)</span>，并且<span class="math inline">\(f(x)\)</span>计算是在多项式时间内可完成的</p>
</blockquote>
<h2 id="uniform-sparest-cut">Uniform Sparest Cut</h2>
<h3 id="problem">problem</h3>
<figure>
<img src="https://s2.loli.net/2023/02/03/1PJrm4LxhgyOeYo.png" alt="image-20230203165146187" /><figcaption aria-hidden="true">image-20230203165146187</figcaption>
</figure>
<p>定义Sparsity of Graph <span class="math display">\[
\rho(S )=\frac{\delta (S)}{|S||V-S|}
\]</span> <span class="math inline">\(\delta(S)\)</span>计算<span class="math inline">\(S,V-S\)</span>之间的边数，要求Sparsity尽量小</p>
<h3 id="引理metric-embedding线性组合构成距离空间">引理：Metric embedding线性组合构成距离空间</h3>
<p>图G的metric embedding的分解</p>
<figure>
<img src="https://s2.loli.net/2023/02/03/FrpSKIUYuinWMjN.png" alt="image-20230203171924088" /><figcaption aria-hidden="true">image-20230203171924088</figcaption>
</figure>
<p>距离函数<span class="math inline">\(\parallel f(u)- f(v)\parallel_1\)</span>可以拆分成不同G的cut indictor function和固定系数<span class="math inline">\(\lambda_S\)</span>内积，m是嵌入空间维度，n是vertex number，至多有<span class="math inline">\(mn\)</span>个<span class="math inline">\(\lambda_S\)</span>是0</p>
<h3 id="linear-program-for-sparest-cut-problem">Linear Program for Sparest Cut Problem</h3>
<p>写成整数线性规划形式</p>
<figure>
<img src="https://s2.loli.net/2023/02/05/csEJadeyHYFkztr.png" alt="image-20230205104721568" /><figcaption aria-hidden="true">image-20230205104721568</figcaption>
</figure>
<h4 id="variable-and-constrain">Variable and constrain</h4>
<ol type="1">
<li><span class="math inline">\(x_e\)</span>每一个edge的指示变量，<span class="math inline">\(x_e=1\)</span>表明e是S和V-S之间的edge</li>
<li><span class="math inline">\(y_{ij}=1\)</span>表明对于节点i和节点j，必然满足<span class="math inline">\(i\in S\and j \in V-S\)</span>或<span class="math inline">\(i\in V-S\and j\in S\)</span>，对<span class="math inline">\(i,j\)</span>求和得到<span class="math inline">\(|V-S||S|\)</span></li>
<li><span class="math inline">\(P_{ij}\)</span>表明i,j之间的所有path，<span class="math inline">\(\sum_{e\in P} x_e \geq y_{ij}\)</span>表明，如果i,j不同时属于S或V-S，则必然至少存在一条跨S,V-S的边</li>
</ol>
<h4 id="relaxation形式">Relaxation形式</h4>
<figure>
<img src="https://s2.loli.net/2023/02/05/txWY7Ef81PyNCZg.png" alt="image-20230205105258114" /><figcaption aria-hidden="true">image-20230205105258114</figcaption>
</figure>
<p>注意LP问题的复杂度是G大小的指数量级(路径数量)，不能用常规的方法求解，这里用到ellipsoid method</p>
<blockquote>
<p>为什么约束<span class="math inline">\(\sum_{i,j}y_{ij}=1\)</span></p>
</blockquote>
<h4 id="algorithm-for-sparest-cut">Algorithm for Sparest Cut</h4>
<ol type="1">
<li><p>求解LP relaxation <span class="math inline">\((x^*,y^*),x^*\in \R^{|E|}\)</span>，<span class="math inline">\(x_e\)</span>是edge e的权重</p></li>
<li><p>定义<span class="math inline">\(d_x(u,j)\)</span>是<span class="math inline">\(i,j\)</span>在图中的最短路径</p></li>
<li><p>得到embedding function，获取metric embedding with distortion <span class="math inline">\(\alpha = O(\log n)\)</span>，使得<span class="math inline">\(f:V\to R^{O(\log ^2 n)}\)</span>，满足 <span class="math display">\[
r d_x (u,v) \leq \parallel f(u)- f(v)\parallel_1\leq r\alpha d_x(u,v),\alpha = O(\log n)
\]</span></p></li>
<li></li>
<li><p>在多项式时间内找到最多<span class="math inline">\(n\log ^2 n\)</span>个non-zero <span class="math inline">\(\lambda_S\)</span>使得距离可以写成<span class="math inline">\(\lambda_S\)</span>和示性函数线性组合的形式 <span class="math display">\[
\parallel f(u) - f(v)\parallel_1 = \sum_{S\subseteq V} \lambda_S \chi_{\delta(S) }(u,v)
\]</span></p></li>
<li></li>
<li><p>输出Sparest Cut <span class="math inline">\(S^*\)</span>，满足<span class="math inline">\(S = \{v|\lambda_S&gt;0,\rho(S)\ is \ \min \},\rho(S^*) =\min _{S:\lambda(S)&gt;0}\rho(S) =\min_{S,\lambda_S&gt;0}\frac{\delta (S)}{|S||V-S|}\)</span></p></li>
</ol>
<blockquote>
<p>这里用到了两个定理</p>
<ol type="1">
<li><code>Theorem 1</code>在n个顶点的图中得到带有distortion <span class="math inline">\(\alpha =O(\log n)\)</span> metric embedding并且embedding dim <span class="math inline">\(m = O(\log^2 n)\)</span>，embedding function大概率可以在多项式时间内完成</li>
<li><code>Lemma 1</code>表明<span class="math inline">\((V,d)\)</span>上的<span class="math inline">\(l_1-\)</span>embeddable metric和associated embedding <span class="math inline">\(f:V\to \R^m\)</span>可以拆分成<span class="math inline">\(\parallel f(u) - f(v)\parallel_1 = \sum_{S\subseteq V}\lambda_S \chi_{\delta(S)}(u,v)\)</span></li>
</ol>
</blockquote>
<h3 id="performance-of-sparest-cut">Performance of Sparest Cut</h3>
<p>randomized <span class="math inline">\(O(\log n)-\)</span>approximation algorithm</p>
<blockquote>
<p>Proof：<span class="math inline">\(\delta (S) = \sum_{e\in E}\chi_{\delta(S)}(e),|S||V- S| = \sum_{i,j}\chi_{\delta(S)}(i,j)\)</span></p>
<blockquote>
<p><span class="math display">\[
\chi_{\delta (S)}(u,v)
\]</span></p>
<p>是indicator function，当<span class="math inline">\(u,v\)</span>跨<span class="math inline">\(S,V-S\)</span>时取1</p>
</blockquote>
<p>知道有<span class="math inline">\(\forall a_i,b_i&gt;0\)</span>，则 <span class="math display">\[
\min_{1\leq i \leq k} \frac{a_i}{b_i}\leq \frac{\sum_{i=1}^k a_i}{\sum_{i=1}^k b_i}\label{inequal}
\]</span> 拆分<span class="math inline">\(\rho(S^*)\)</span> <span class="math display">\[
\begin{align}
\rho(S^*) &amp;= \min_{S,\lambda_S&gt;0}\frac{\delta(S)}{|S||V-S|}\\
&amp;=\min_{S,\lambda_S&gt;0}\frac{\sum_{e\in E}\chi_{\delta(S)}(e)}{\sum_{i,j}\chi_{\delta(S)}(i,j)}\\
&amp;\leq \frac{\sum_{S\subseteq V}\lambda_S \sum_{e\in E}\chi_{\delta(S)}(e)}{\sum_{S\subseteq V}\lambda _S \sum_{i,j}\chi_{\delta (S)}(i,j)}\\
&amp;=\frac{\sum_{e\in E}\sum_{S\subseteq V}\lambda_S \chi_{\delta(S)}(e)}{\sum_{i,j}\sum_{S\subseteq V}\lambda_S \chi_{\delta(S)}(i,j)}\\
&amp;=\frac{\sum_{e=(u,v) \in E}\parallel f(u)-f(v)\parallel_1}{\sum_{i,j} \parallel f(i) - f(j)\parallel_1}\\
&amp;\leq \frac{r\cdot O(\log n) \sum_{e= (u,v)\in E}d_x(u,v)}{r\cdot \sum_{i,j}d_x(i,j)}
\end{align}
\]</span></p>
<ol type="1">
<li>(6)来自<span class="math inline">\(\ref{inequal}\)</span></li>
<li>(9)来自求和号更换顺序</li>
<li>(10)来自Lemma 1</li>
<li>(11)来自<span class="math inline">\(\parallel f(u) - f(v)\parallel_1\leq r O(\log n) d_x(u,v)\)</span></li>
</ol>
<p>进一步写出 <span class="math display">\[
\begin{align}
\rho(S^*) &amp;=O(\log n)\frac{\sum_{e=(u,v)\in E}d_x(u,v)}{\sum_{i,j}d_x(i,j)}\\
&amp;\leq O(\log n)\frac{\sum_{e\in E}x_e}{\sum_{i,j} y_{ij}}\\
&amp;=O(\log n)\sum_{e\in E} x_e\\
&amp;\leq O(\log n)OPT_{LP}\leq O(\log n) OPT
\end{align}
\]</span></p>
<ol type="1">
<li>(13)来自<span class="math inline">\(x_e\geq d_x(u,v),(u,v)\in E\)</span>，并且LP constrain <span class="math inline">\(y_{ij}\leq d_x(i,j)\)</span></li>
</ol>
<blockquote>
<p><span class="math inline">\(x_e\geq d_x(u,v)\)</span>是因为<span class="math inline">\(x_e\)</span>是边权并且<span class="math inline">\(d_x\)</span>定义为shortest path</p>
</blockquote>
<ol type="1">
<li><span class="math inline">\(14\)</span>来自约束<span class="math inline">\(\sum_{i,j} y_{ij}=1\)</span></li>
<li>最后用到了<span class="math inline">\(OPT_{LP}\leq OPT\)</span></li>
</ol>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/19/Approximation-Algorithm-Sparest-Cut/" data-id="cleze2ot90000sg8wa1le8xvv" data-title="Approximation Algorithm Sparest Cut" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithm-design-and-Analysis-Approximation-Algorithm/" rel="tag">Algorithm design and Analysis, Approximation Algorithm</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Approximation-Algorithm-TSP-Max-2-SAT" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/19/Approximation-Algorithm-TSP-Max-2-SAT/" class="article-date">
  <time class="dt-published" datetime="2023-02-19T03:09:23.000Z" itemprop="datePublished">2023-02-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/19/Approximation-Algorithm-TSP-Max-2-SAT/">Approximation Algorithm TSP Max 2-SAT</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="approximation-algorithmtspmax-2-sat-and-k-center">Approximation Algorithm(TSP,Max 2-SAT and k-center)</h1>
<h2 id="tsp">TSP</h2>
<h3 id="problem">Problem</h3>
<p>无向带权图中，寻找环路遍历每个顶点，使得距离最小，权重<span class="math inline">\(w(u,v)\)</span>满足 <span class="math display">\[
w(u,v)\leq w(u,x)+w(x,v)
\]</span></p>
<h3 id="algorithmfindtour">algorithm(FINDTOUR)</h3>
<ol type="1">
<li>找到graph的一个最小生成树T</li>
<li>在T上重复每个边两次，得到H</li>
<li>在H上找到一条Eulerian回路C</li>
<li>构建TSP，如果C上遍历一个vertex两次，则跳过这个vertex直接visit下一个vertex</li>
</ol>
<h4 id="approximation-algorithm">2-Approximation algorithm</h4>
<p><span class="math inline">\(C^*\)</span>是optimal tour的开销，必然有 <span class="math display">\[
MST\leq C^*
\]</span> 这是因为<span class="math inline">\(C^*\)</span>移除一条边必然构成G的生成树，因此 <span class="math display">\[
Cost(Eulerian\ tour)\leq 2C^*\\
Cost(final\ output)\leq 2C^*
\]</span></p>
<h3 id="approximation-algorithm-of-tsp">1.5 approximation algorithm of TSP</h3>
<h4 id="algorithm">Algorithm</h4>
<ol type="1">
<li>获得MST T</li>
<li>在MST的偶数度数顶点中计算最小代价完全匹配M，将M加入到T中获得欧拉图H</li>
<li>在H上获得欧拉回路C</li>
<li>基于FINDTOUR相同的算法从C构建出一条欧拉回路</li>
</ol>
<h4 id="引理偶数顶点minimum-cost-perfect-matching的代价">引理：偶数顶点minimum cost perfect matching的代价</h4>
<p>设<span class="math inline">\(V^\prime\subseteq V,|V^\prime|\)</span>为偶数，M是<span class="math inline">\(V^\prime\)</span>上的minimum cost perfect matching，则确定M cost上界 <span class="math display">\[
cost(M)\leq \frac{C^*}{2}
\]</span></p>
<blockquote>
<p>proof，在一个cost为<span class="math inline">\(C^*\)</span>的最优TSP tour <span class="math inline">\(T^\prime\)</span>上去掉<span class="math inline">\(V-V^\prime\)</span>(做shortcut，和FINDTOUR相同)得到新的在<span class="math inline">\(V^\prime\)</span>上的回路<span class="math inline">\(T\)</span>，必然有 $$ cost(T)cost(T^) = C^*</p>
</blockquote>
<blockquote>
<p>考虑<span class="math inline">\(V^\prime\)</span>组成的环路，记作 <span class="math display">\[
v_1,v_2,\cdots,v_{2k},v_1
\]</span> 则取一个match <span class="math display">\[
(v_1,v_2),(v_3,v_4),\cdots,(v_{2k-1},v_{2k})
\]</span> WLOG，假设这个match M的cost满足 <span class="math display">\[
2 cost(M)\leq cost(T^\prime) = C^*
\]</span> &gt; 如果不成立，则<span class="math inline">\((v_2,v_3),(v_4,v_5),\cdots,(v_{2k},v_1)\)</span>的cost必然<span class="math inline">\(\leq \frac{C^*}{2}\)</span></p>
<p>基于上述讨论，有 <span class="math display">\[
cost(M+MST)\leq \frac{3}{2}C^*
\]</span> Q.E.D</p>
</blockquote>
<h2 id="max-2-sat">MAX 2-SAT</h2>
<h3 id="problem-1">Problem</h3>
<p>给定n个bool变量<span class="math inline">\(x_1,x_2,\cdots,x_n\)</span>和m个合取范式<span class="math inline">\(C_1,C_2,\cdots,C_m\)</span></p>
<blockquote>
<p>每个clause是两个子句<span class="math inline">\(t_1,t_2\)</span>的或，子句采样自<span class="math inline">\(x_1,x_2,\cdots,x_n,\not {x_1},\not x_2,\cdots,\not x_n\)</span></p>
</blockquote>
<p>找到变量赋值使得被满足的合取范式最多</p>
<h3 id="randomassign">RANDOMASSIGN</h3>
<p>随机给变量赋值，是<span class="math inline">\(\frac{3}{4}\)</span>近似最优算法，对于每个子句<span class="math inline">\(Y_i\)</span> <span class="math display">\[
Pr(Y_i=1) = 1-Pr(Y_i = 0) = \frac{3}{4}
\]</span> 所有子句 <span class="math display">\[
E[Y] = \sum_{i=1}^m E[Y_i] = \frac{3}{4}m\geq \frac 3 4 OPT
\]</span></p>
<h2 id="complex-theory">Complex Theory</h2>
<h3 id="可解决问题">可解决问题</h3>
<p>定义：P为所有在多项式时间内可以解决的问题集合</p>
<h3 id="多项式规约">多项式规约</h3>
<p>可以在多项式时间内将问题X转化为问题Y，指的是存在一个多项式算法，可以将每个X的实例x通过算法<span class="math inline">\(\mathcal A\)</span>在多项式事件内转化为Y中的instance y，并满足</p>
<figure>
<img src="https://s2.loli.net/2023/02/09/J3vrqXAfKgZhVLn.png" alt="image-20230209163631473" /><figcaption aria-hidden="true">image-20230209163631473</figcaption>
</figure>
<p>若 <span class="math display">\[
X\leq_P Y
\]</span></p>
<ol type="1">
<li>Y存在多项式算法则X必然存在多项式算法</li>
<li>X不存在多项式算法则Y必然不存在多项式算法</li>
</ol>
<p>X可以在多项式时间内规约成Y</p>
<h3 id="p类问题np问题和pnp">P类问题，NP问题和P=NP</h3>
<ol type="1">
<li>P类问题：一个可以在多项式时间内找到solution的问题</li>
<li>NP问题：可以在多项式时间内验证一个解(多项式时间内猜出一个解)</li>
</ol>
<blockquote>
<ol type="1">
<li>在一个图中找出Hamilton回路(NP问题)</li>
<li>是否存在Hamilton回路(不是NP问题)</li>
</ol>
<p>所有P问题都是NP问题，必然有<span class="math inline">\(P\subseteq NP\)</span></p>
</blockquote>
<h3 id="npc问题">NPC问题</h3>
<blockquote>
<p>Hamiton问题在一个非完全图上找回路，TSP是在完全带权图上找回图，Hamition转化为TSP则是在带权图(真正在G中相邻权重为0，否则为1)是否存在权重为0的葫芦</p>
</blockquote>
<p>问题多项式约化<span class="math inline">\(X\leq_{P} Y\)</span>表明问题Y的难度一定不会超过X，NPC问题解决的是，是否存在一类问题，使得所有的NP问题都可以约化为它，如果解决这类问题则所有NP问题都可以解决，一个NPC问题需要满足如下条件</p>
<ol type="1">
<li>是一个NP问题<span class="math inline">\(NPC\subseteq NP\)</span></li>
<li>一个已知的NPC问题可以归约到它</li>
</ol>
<blockquote>
<p>如何找到<strong>第一个</strong>NPC问题？</p>
</blockquote>
<h3 id="nphard问题">NPHard问题</h3>
<p>满足第二条但是不满足第一条</p>
<h3 id="第一个npc问题逻辑电路">第一个NPC问题：逻辑电路</h3>
<p>给定逻辑电路，是否存在一种输入使得输出为True</p>
<blockquote>
<p>任何一个NPC问题可以归约到此</p>
</blockquote>
<blockquote>
<p>An example of Reduction <span class="math inline">\(IS=_P VC\)</span></p>
<p><code>Independent Set</code> and <code>Vertex Cover</code>，前者描述G中存在最多多少点两两不相交(是否存在大小不少于为k的Independent Set?)，后者描述的是最小需要多少点可以使得每个边至少被一个顶点关联(是否存在大小不超过k的vertex cover)，只需要证明</p>
<ol type="1">
<li>如果X是一个vertex cover，则<span class="math inline">\(V-X\)</span>是一个独立集合</li>
<li>如果X是一个Independent set，则<span class="math inline">\(V-X\)</span>是一个vertex cover</li>
</ol>
<p>Another example of Reduction <span class="math inline">\(VC\leq SC(Set\ Cover)\)</span></p>
<p>Set Cover描述的是对于一个全集U和它的若干子集<span class="math inline">\(S_1,S_2,\cdots,S_m\)</span>，是否存在不多于k个子集使得它们的并等于U？Vertex Cover的一个instance记作<span class="math inline">\((G,k)\)</span>，一个Vertex Cover的Set Cover instance包括</p>
<p>图的每个边映射为全集U中的一个元素，每个顶点映射为一个subset，它的元素是它关联的边</p>
</blockquote>
<h3 id="可满足和3-可满足">可满足和3-可满足</h3>
<ol type="1">
<li>可满足：逻辑电路(一个Conjunctive Normal Form的布尔函数是否是可满足的，Conjuctive Normal Form写成若干子句的且，每个子句是literal的或操作)</li>
<li>3-可满足，每个子句仅包括3个literal的或操作</li>
</ol>
<p>定理 <span class="math display">\[
3-SAT\leq_P IS
\]</span> 3-满足问题经过多项式事件可以规约为最大独立集 <span class="math display">\[
包含n个变量，m个clauses的3-SAT问题\\{\Rightarrow}是否可满足^{P规约}构造一个图，包含3m个节点，是否存在大小为m的独立集合
\]</span></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/19/Approximation-Algorithm-TSP-Max-2-SAT/" data-id="cleze2otf0001sg8w532db731" data-title="Approximation Algorithm TSP Max 2-SAT" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithm-design-and-Analysis-Approximation-Algorithm/" rel="tag">Algorithm design and Analysis, Approximation Algorithm</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Approximation-Algorithm-Maxcut-and-Correlation-Cluster" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/19/Approximation-Algorithm-Maxcut-and-Correlation-Cluster/" class="article-date">
  <time class="dt-published" datetime="2023-02-19T03:07:34.000Z" itemprop="datePublished">2023-02-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/19/Approximation-Algorithm-Maxcut-and-Correlation-Cluster/">Approximation Algorithm Maxcut and Correlation Cluster</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="algorithm-review-approximation-algorithm-part5max-cut-and-correlation-clustering">Algorithm review Approximation Algorithm Part5(Max cut and correlation clustering)</h1>
<h2 id="approximation-rate">Approximation rate</h2>
<p>主要有两种对Approximation Rate的讨论</p>
<h3 id="approximation-in-expectation">Approximation in expectation</h3>
<p><span class="math display">\[
E[\frac{Alg}{OPT}] \leq(\geq) \alpha
\]</span></p>
<p>对于同一个问题，Alg运行的结果带有随机性，结果的期望和最优解之比被bound</p>
<h3 id="approximation-with-high-probability">Approximation with high probability</h3>
<p>从概率角度，算法输出和真实值之比被<span class="math inline">\(\alpha\)</span>约束的概率很大 <span class="math display">\[
Pr[\frac{Alg}{OPT}\leq(\geq)\alpha]&gt;1-\alpha
\]</span></p>
<h3 id="approximation-in-expected-running-time">Approximation in expected running time</h3>
<p>始终满足 <span class="math display">\[
\frac{Alg}{OPT}\leq (\geq )\alpha
\]</span> 但是Alg运行时间可能极差</p>
<h2 id="max-cut">Max Cut</h2>
<p>给定无向带权图<span class="math inline">\(G=(V,E,w),w_{ij}\geq 0\)</span>，希望找到顶点划分<span class="math inline">\(S,V-E\)</span>，使得cut权重尽量大</p>
<h3 id="semidefinite-programming">Semidefinite Programming</h3>
<p>给定<span class="math inline">\(X\in \R^{n\times n}\)</span>的半正定对称矩阵，一定存在分解 <span class="math display">\[
X=U^T U,U\in \R^{k\times n}
\]</span> 两个等价定义，定义1 <span class="math display">\[
\max \sum_{ij} w_{ij} x_{ij}\\
s.t\quad \sum_{ij} a_{ij}^r x_{ij} \geq b_r,r\in I\\
X\succeq 0
\]</span> 定义2(基于矩阵分解) <span class="math display">\[
\max \sum_{ij}w_{ij}\lang u_i,u_j\rang\\
s.t\quad \sum_{ij} a_{ij}^r \lang u_i,u_j\rang \geq b_r,r\in I\\
u_i\in \R^n
\]</span></p>
<h3 id="sdp-for-max-cut">SDP For Max Cut</h3>
定义 $$ v_i ={
<span class="math display">\[\begin{aligned}

&amp; e,i\in S\\
&amp; -e,i\notin S

\end{aligned}\]</span>
<p>. $$ e是一个0-1单位向量，<span class="math inline">\(|e| =|V|\)</span></p>
<figure>
<img src="https://s2.loli.net/2023/01/31/Av16GpfRhx3jOFu.png" alt="image-20230131173249369" /><figcaption aria-hidden="true">image-20230131173249369</figcaption>
</figure>
<h3 id="sdp-for-max-cut-1">SDP for Max Cut</h3>
<ol type="1">
<li>计算最优的Max cut <span class="math inline">\(v_i\)</span></li>
<li>随机选择单位向量r
<ol type="1">
<li><span class="math inline">\(x_1,x_2,\cdots,x_n|\mathcal N(0,1)\)</span></li>
<li><span class="math inline">\(r = \frac{1}{\sqrt{x_1^2+x_2^2+\cdots+x_n^2}}(x_1,x_2,\cdots,x_n)\)</span></li>
</ol></li>
<li><span class="math inline">\(S = \{i\in V|\lang r,v_i\rang \geq 0 \}\)</span></li>
</ol>
<h4 id="引理分割定理">引理：分割定理</h4>
<blockquote>
<p>单位球上任取两个向量<span class="math inline">\(v_i,v_j\)</span>，再任取一个单位向量<span class="math inline">\(r\)</span>，记事件A,B <span class="math display">\[
A : \lang v_i,r\rang \geq 0\\
B : \lang v_j,r\rang \geq 0
\]</span> &gt; r在单位球上分割<span class="math inline">\(v_i,v_j\)</span></p>
<p>考虑<span class="math inline">\(Pr(A\bigcap B)\)</span>，对于<span class="math inline">\(v_i,v_j\)</span>构成的圆，他们之间的夹角为 <span class="math display">\[
\theta = \arccos (\lang v_i,v_j \rang)\\
Pr(A\bigcap B) =\frac{2\theta}{2\pi}=\frac{1-\lang v_i,v_j\rang}{\pi}\frac{\arccos(\lang v_i,v_j\rang)}{1-\lang v_i,v_j\rang}
\]</span> 令 <span class="math display">\[
f(x) = \frac{\arccos x}{1-x},\frac{2}{\pi}\min _x f(x) = 0.878 
\]</span> 因此 <span class="math display">\[
Pr(A\bigcap B)\geq 0.878 \frac{1-\lang v_i,v_j \rang}{2}
\]</span></p>
</blockquote>
<h4 id="定理算法最优性">定理：算法最优性</h4>
<p>根据SDP-MAXCut求解的解记作<span class="math inline">\(OPT_{SDP}\)</span>，它与最优解的关系满足 <span class="math display">\[
OPT_{SDP}\geq 0.878 OPT
\]</span></p>
<p>显然round后的最优解一定比Max-cut原问题更优，即 <span class="math display">\[
\sum_{(i,j)\in E} \frac{1}{2}w_{ij}(1-\lang v_i^*,v_j^*\rang)\geq OPT
\]</span> 实际转化为Max-cut后目标解写成 <span class="math display">\[
\sum_{(i,j)\in E,\lang v_i^*,r\rang \times \lang v_j^*,r\rang \leq 0} \frac{1}{2}w_{ij}
\]</span> <span class="math inline">\(\lang v_i^*,r\rang\times \lang v_j^*,r\rang \leq 0\)</span>表示<span class="math inline">\(i\in S\)</span>和<span class="math inline">\(j\in S\)</span>不同时成立，同时根据引理，对于任意两个顶点<span class="math inline">\(i,j\)</span>，他们同时属于<span class="math inline">\(S\)</span>的概率为 <span class="math display">\[
Pr(i,j\in S=False) \geq 0.878 \frac{1-v_i^T v_j}{2}
\]</span> 写成 <span class="math display">\[
\sum_{(i,j)\in E}w_{ij} \frac{1-\lang v_i^*,v_j\rang}{2}\leq \frac{1}{0.878} \sum_{(i,j)\in }w_{ij}Pr(i,j\in S=False)=\frac{1}{0.878}E[alg]
\]</span> 这个算法是<strong>期望上近似</strong>的Approximation Algorithm</p>
<h2 id="correlation-clusterv1">Correlation cluster(v1)</h2>
<h3 id="question">Question</h3>
<p>给定<span class="math inline">\(G=(V,E)\)</span>，<span class="math inline">\(\forall e\in E\)</span>映射为<span class="math inline">\(e\to +\or -\)</span>，要求找到t个partition <span class="math inline">\(P=(P_1,P_2,\cdots,P_t)\)</span>，使得跨cluster之间的positive edges和cluster内的negative egdes之和最大</p>
<h3 id="from-correlation-cluster-to-sdp">from correlation cluster to SDP</h3>
<p>定义<code>sgn</code>函数 <span class="math display">\[
\forall(i,j)\in E,sgn(ij) =\left\{
\begin{aligned}
&amp;1,(i,j)\to +\\
&amp;-1,(i,j)\to -
\end{aligned}
\right.
\]</span> 对于包含n个结点的图，定义$e_{k}{0,1}^n <span class="math inline">\(且\)</span>e_k$是单位向量(第k维为1，其余为0)</p>
<blockquote>
<p>这里讲义中是否存在问题，貌似<span class="math inline">\(e_k\in \{0,1\}^t\)</span>更加合理</p>
</blockquote>
<p>写成SDP <span class="math display">\[
\max\sum_{i&lt;j,sgn(ij)=1}x_i^T x_j +\sum_{i&lt;j,sgn(ij)=-1}(1-x_i^Tx_j)\\
s.t\quad x_i\in \{e_1,e_2,\cdots,e_n\},i\in V
\]</span></p>
<ol type="1">
<li><span class="math inline">\(i&lt;j\)</span>是因为无向图</li>
<li><span class="math inline">\(x_i\)</span>作为标志每个节点的indicate variable</li>
</ol>
<p>SDP relaxation形式，约束变成 <span class="math display">\[
x_i^T x_i = 1,x_i^Tx_j\geq 0,x_i\in \R^n,i,j\in V
\]</span></p>
<h3 id="algorithm-sdp-cc">Algorithm: SDP-CC</h3>
<p>随机选择两个单位球上的向量<span class="math inline">\(r_1,r_2\)</span>，将空间分成4个部分</p>
<figure>
<img src="https://s2.loli.net/2023/02/02/1wy8J7XkPLfCohg.png" alt="image-20230202165015295" /><figcaption aria-hidden="true">image-20230202165015295</figcaption>
</figure>
<p>对于<span class="math inline">\(\forall (i,j)\in E\)</span>，定义<span class="math inline">\(q_{ij}\)</span>为<span class="math inline">\(v_i,v_j\)</span>落在两个hyperplane同一侧的概率，显然 <span class="math display">\[
q_{ij} = (1-\frac{\arccos v_i^T v_j}{\pi})^2\Leftrightarrow i\in R_1\bigcup R_4,j\in R_1\bigcup R_4
\]</span> 加上平方项是因为有两个超平面，算法运行的期望为 <span class="math display">\[
\sum_{i&lt;j,sgn(i,j)=1}q_{ij}+\sum_{i&lt;j,sgn(i,j)=-1}(1-q_{ij})
\]</span> 显然有 <span class="math display">\[
\alpha_1 = \min_z \frac{(1-\frac{\arccos(z)}{\pi})^2}{z},\alpha_2 =\min_z \frac{(1-(1-\frac{\arccos z}{\pi})^2)}{1-z},\alpha^*=\min{\alpha_1,\alpha_2}\geq 0.75
\]</span> Round结果显然比之前更优，因此 <span class="math display">\[
\sum_{i&lt;j,sgn(ij)=1}v_i^* v_j +\sum_{i&lt;j,sgn(ij)=-1} (1-v_i^*v_j)\geq OPT
\]</span> 得到算法是0.75近似</p>
<h2 id="correlation-clusteringv2">Correlation clustering(v2)</h2>
<h3 id="problem">Problem</h3>
<p>希望disagreement of G最小，disagreement定义为</p>
<blockquote>
<p>节点聚类<span class="math inline">\(P=(P_1,P_2,\cdots,P_t)\)</span>中聚类内部的negative edges和聚类之间的positive edges之和</p>
</blockquote>
<h3 id="algorithm-pivot-cc">Algorithm: PIVOT-CC</h3>
<figure>
<img src="https://s2.loli.net/2023/02/02/G9Z42oMhJUnxIqm.png" alt="image-20230202174334789" /><figcaption aria-hidden="true">image-20230202174334789</figcaption>
</figure>
<p>不断去除节点的positive neighbor</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/19/Approximation-Algorithm-Maxcut-and-Correlation-Cluster/" data-id="cleze2oti0003sg8wce9lc8vo" data-title="Approximation Algorithm Maxcut and Correlation Cluster" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithm-design-and-Analysis-Approximation-Algorithm/" rel="tag">Algorithm design and Analysis, Approximation Algorithm</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-hardness-of-approximation" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/19/hardness-of-approximation/" class="article-date">
  <time class="dt-published" datetime="2023-02-19T03:04:01.000Z" itemprop="datePublished">2023-02-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/19/hardness-of-approximation/">hardness of approximation</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="approximation-algorithmhardness-of-approximation">Approximation Algorithm(Hardness of Approximation)</h1>
<h2 id="hardness-of-approximation">Hardness of Approximation</h2>
<p>近似算法复杂度仍然太高</p>
<h3 id="gap-reduction">gap reduction</h3>
<p>定义NP到P的<span class="math inline">\((f,c_1,c_2)\)</span> gap reduction，将问题L映射为问题<span class="math inline">\(\Pi\)</span>(最小化问题)</p>
<blockquote>
<p>L是NPC判定问题，<span class="math inline">\(\Pi\)</span>是NPH最小化问题，从NPC问题构造最优化问题，NPC问题只有Yes instance和No instance</p>
</blockquote>
<ol type="1">
<li>f是problem instance之间的映射(规约)</li>
<li><span class="math inline">\(c_1,c_2\)</span>是<span class="math inline">\(N\to Q^+\)</span>的映射函数(复杂性，最优性)</li>
<li><span class="math inline">\(f,c_1,c_2\)</span>都是多项式复杂性函数</li>
<li><span class="math inline">\(x\in L,OPT(f(x))\leq c_1 |f(x)|\)</span>（输入是Yes instance，<span class="math inline">\(|f(x)|\)</span>表示reduction大小）</li>
<li><span class="math inline">\(x\notin L,OPT(f(x))&gt; c_2 |f(x)|\)</span>（输入时No instance，OPT不可能太小）</li>
</ol>
<blockquote>
<p>假设最优化问题<span class="math inline">\(\Pi\)</span>存在近似比为<span class="math inline">\(\frac{c_2}{c_1}\)</span>的近似比算法<span class="math inline">\(\mathcal {A}\)</span>，对于每个L的实例<span class="math inline">\(l\in L\)</span>，构造出<span class="math inline">\(\Pi\)</span>中的实例<span class="math inline">\(I^\prime\)</span>，两种情况 $$ OPT(I^)c_1 |I<sup>|A(I</sup>)OPT(I^)c_2 |I^|IL(Yes instance)\ OPT(I^)c_2 |I<sup>|A(I</sup>)c_2 |I^|IL(No instance)</p>
</blockquote>
<blockquote>
<p>$$ 则NPC被解决<span class="math inline">\(\mathcal A+reduction\)</span>，这和假设<span class="math inline">\(NP\neq P\)</span>矛盾，因此在<span class="math inline">\(NP\neq P\)</span>假设下，不存在<span class="math inline">\(\frac{c_2}{c_1}\)</span>近似算法</p>
</blockquote>
<p>Hardness of Approximation实际上描述了不同近似问题具有的近似比的性质，显然常数近似，指数近似，对数近似不是一类问题</p>
<h2 id="k-center-problem">K-center problem</h2>
<h3 id="problem">Problem</h3>
<p>无向完全图<span class="math inline">\(G=(V,E)\)</span>，定义边权<span class="math inline">\(d_{ij}\)</span>满足距离性质<span class="math inline">\(d_{ij}\leq d_{ik}+ d_{kj}\)</span></p>
<blockquote>
<p>找到大小为k的子集S，使得cost of S最小，cost定义为 <span class="math display">\[
cost(S) = \max_{v\in V}  \min_{s\in S} d_{is}
\]</span> 定义为每个点到S距离的最大值</p>
</blockquote>
<p>已知存在2-approximation算法（<code>GREADYCENTER</code>，每次寻找距离S最大的节点加入S），证明<code>P=NP</code>假设下的一个推论</p>
<h3 id="定理假设pneq-np则对forall-epsilon-0k-center不存在2-epsilon近似算法">定理：假设<span class="math inline">\(P\neq NP\)</span>，则对<span class="math inline">\(\forall \epsilon &gt;0\)</span>，k-center不存在<span class="math inline">\((2-\epsilon)\)</span>近似算法</h3>
<h4 id="定义dspnpcdominated-set">定义DSP(NPC,dominated set)</h4>
<blockquote>
<p>NPH不一定能在多项式时间内验证解的正确性，因此不一定属于NP，和NPC不同，后者一定是NP的子集</p>
</blockquote>
<figure>
<img src="https://s2.loli.net/2023/02/16/nHMbhZRf7CYKUoG.png" alt="image-20230216154813881" /><figcaption aria-hidden="true">image-20230216154813881</figcaption>
</figure>
<blockquote>
<p>注意这里的规约方向是<span class="math inline">\(DSP\to k-center\)</span>，只要解决k-center就一定能解决DSP</p>
</blockquote>
<p>所有节点要么在S中要么在S的相邻节点中，给定一个DSP的instance <span class="math inline">\(\lang G=(V,E),k\rang\)</span>，构造对应的k-center problem实例 <span class="math inline">\(\lang G = (V,E^\prime),d_{ij},k\rang\)</span></p>
<blockquote>
<p><span class="math inline">\((i,j)\in E\to d_{ij }=1\)</span>否则<span class="math inline">\(d_{ij}=2\)</span></p>
</blockquote>
<p>Dominate Set S存在$$Optimal radius of k-center problem H is 1</p>
<blockquote>
<p>存在S，则V-S的任何节点均和S相邻，<span class="math inline">\(\forall v\in V-S ,d_{v S} = 1\)</span>(权重定义)</p>
</blockquote>
<p>不存在<span class="math inline">\((2-\epsilon)\)</span>近似，因为任何近似算法输出为1或2</p>
<blockquote>
<p>考虑带权完全图，边权<span class="math inline">\(w\in \{1,2\}\)</span>，求大小为S的点子集，使得 <span class="math display">\[
cost(S) =\max_{v\in V} d_{vS}
\]</span> 最小。显然，<span class="math inline">\(\forall v\in V-S ,d_{vS} = 1/2\)</span>，假设DSP的回答是yes，则必然有<span class="math inline">\(cost(S) = 1\)</span>。如果存在<span class="math inline">\((2-\epsilon)\)</span>近似，则求出的近似解<span class="math inline">\(S^*\)</span>满足 <span class="math display">\[
\frac{cost(S^*)}{cost(S)}\leq 2-\epsilon
\]</span> 则<span class="math inline">\(cost(S^*)=1\)</span>，即求解<span class="math inline">\(DSP\)</span> reduction问题，通过最优解可以判断原问题解是否存在（如果<span class="math inline">\(cost(S^*)=1\)</span>则存在DSP集合S，否则不存在）并且求出的集合<span class="math inline">\(S^*\)</span>就是DSP的解(可以在多项式时间内判断输入<span class="math inline">\(S\)</span>时DSP是否回答Yes)，这和NPH矛盾</p>
</blockquote>
<p><span class="math display">\[
c_1(n)=1,c_2(n)=2-\epsilon
\]</span></p>
<p>这是因为必然有 <span class="math display">\[
cost(DSP)\leq cost(f(DSP))
\]</span></p>
<h2 id="bin-packing-problem">Bin Packing Problem</h2>
<h3 id="problem-1">Problem</h3>
<p>给定包含n个item的集合，每个item <span class="math inline">\(o_i\)</span>具有size <span class="math inline">\(s_i\in(0,1]\)</span>，定义包含n个bins的集合B，每个bin的容量是1</p>
<blockquote>
<p>将item放到不同bins中，满足</p>
<ol type="1">
<li>每个bins能承载所属的item</li>
<li>使用尽量少的bins(即将多个item尽量放在一个bin中)，一个bin放一个item一定放得下，但是太浪费了</li>
</ol>
<blockquote>
<p>找到item到bin的映射</p>
</blockquote>
</blockquote>
<h3 id="firstfit">FIRSTFIT</h3>
<ol type="1">
<li>按照顺序依次放置item，放不下则转移到下一个bin</li>
<li>下一个item来也需要从第一个bin开始遍历，找到能放下的bin</li>
</ol>
<h4 id="近似算法">2-近似算法</h4>
<blockquote>
<p>假设<code>FIRSTFIT</code>需要m个bins，则至少m-1个bins花了至少一半的capacity装items（不可能有两个都没装到一半，否则可以合并为1个），定义这些bin为good bins。最优解OPT必然有 <span class="math display">\[
OPT\geq \sum_{i=1}^n s_i
\]</span> 必然有 <span class="math display">\[
\sum_{i=1}^n s_i \geq \sum_{i\to good\ bins} s_i&gt;\frac{m-1}{2}
\]</span> 得证，关键使用<span class="math inline">\(\sum s_i\)</span>作为桥梁沟通最优解和近似解</p>
</blockquote>
<h3 id="定理如果pneq-np对于bin-packing-problem不存在frac32-epsilon近似算法">定理：如果<span class="math inline">\(P\neq NP\)</span>，对于bin packing problem不存在<span class="math inline">\(\frac{3}{2}-\epsilon\)</span>近似算法</h3>
<h4 id="partition-problem">Partition Problem</h4>
<p>给定一个包含n个数的set，<span class="math inline">\(c_i\)</span>是整数</p>
<blockquote>
<p>是否存在1-n的下标子集S <span class="math inline">\(S\in \{1,2,\cdots,n\}\)</span>，使得 <span class="math display">\[
\sum_{i\in S} c_i = \sum_{i\neq S} c_i
\]</span> 假设每个数不会超过总和的一半，不然问题显然是No</p>
</blockquote>
<h4 id="reduction-partitionto-bin-packing">Reduction <span class="math inline">\(Partition\to Bin \ Packing\)</span></h4>
<p>对于包含n个数的partition instance <span class="math inline">\(I=(c_1,c_2,\cdots,c_n)\)</span>，定义它reduction生成的Bin Packing instance <span class="math inline">\(I^\prime\)</span>:item <span class="math inline">\(o_i,s_i=\frac{2c_i}{C}\)</span>，给定问题能否将Bin Packing问题放在两个bins中</p>
<ol type="1">
<li>假设S是partition instance <span class="math inline">\(I\)</span>，则可以将<span class="math inline">\(I^\prime\)</span>放置到两个bins</li>
<li><span class="math inline">\(I\)</span>如果无解则<span class="math inline">\(I^\prime\)</span>至少需要3个bins</li>
</ol>
<blockquote>
<p>同样，逻辑上是 <span class="math display">\[
存在\frac{3}{2}-\epsilon近似\to 可以在多项式时间内解决Partition问题
\]</span></p>
<ol type="1">
<li>如果<span class="math inline">\(I\)</span>是一个Yes instance，近似解满足<span class="math inline">\((\frac{3}{2} -\epsilon)\cdot 2 &lt; 3\)</span>，必然得到2 bins</li>
<li>如果<span class="math inline">\(\frac{3}{2}-\epsilon\)</span>算法解出至少要放在3 bins，则<span class="math inline">\(I\to No\)</span></li>
</ol>
<p>因为partition是一个NPH问题，这和<span class="math inline">\(P\neq NP\)</span>假设矛盾</p>
</blockquote>
<blockquote>
<p>这两个例子都告诉我们，<span class="math inline">\(NP\neq P\)</span>假设下，2近似或<span class="math inline">\(\frac{3}{2}\)</span>近似已经是最好的近似算法。证明思路类似，都是从一个NPC问题判断规约到当前最优化近似问题</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/19/hardness-of-approximation/" data-id="cleze2otn0009sg8wge342741" data-title="hardness of approximation" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithm-design-and-Analysis-Approximation-Algorithm/" rel="tag">Algorithm design and Analysis, Approximation Algorithm</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Dirichletprocess1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/17/Dirichletprocess1/" class="article-date">
  <time class="dt-published" datetime="2023-02-17T11:33:08.000Z" itemprop="datePublished">2023-02-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/17/Dirichletprocess1/">Dirichletprocess1</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="dirichlet-process">Dirichlet Process</h1>
<h2 id="motivation">Motivation</h2>
<p>给定若干采样自GMM二维数据，希望聚类为若干类(求解GMM中的k) <span class="math display">\[
\log P(x) = \log \sum_{i=1}^k \alpha_i N(x|\mu_i,\sum_i)
\]</span></p>
<blockquote>
<p>每个instance属于一类最好，但是这个结果没有意义</p>
</blockquote>
<p>每个数据<span class="math inline">\(x_i\)</span>来自分布<span class="math inline">\(p(\theta_i),\theta_i\)</span>为分布的参数，并且 <span class="math display">\[
\theta_i |H(\theta)
\]</span></p>
<blockquote>
<p>参数的分布来自另一个分布，若<span class="math inline">\(H\)</span>是连续分布，则 <span class="math display">\[
H(\theta_1=\theta_2) = 0
\]</span> 因此得到N个分布，这也是不符合预期的，假设 <span class="math display">\[
\theta_i|G
\]</span> G是一个离散分布，并且 <span class="math display">\[
G|DP(\alpha,H),\alpha &gt;0
\]</span> 满足初始分布为H，参数为<span class="math inline">\(\alpha\)</span>的Dirichlet Process</p>
<ol type="1">
<li><span class="math inline">\(\alpha\)</span>控制分布的离散程度，<span class="math inline">\(\alpha\)</span>越大则G不那么离散，极端期望下<span class="math inline">\(\alpha = 0\)</span> G变成单点分布</li>
<li>H被称为Base measure</li>
</ol>
</blockquote>
<h2 id="construction-of-dirichlet-process">Construction of Dirichlet Process</h2>
<p>存在分布<span class="math inline">\(x|p(x)\to H(\theta)\)</span></p>
<h3 id="stick-breaking">Stick Breaking</h3>
<p>通过Stick Break采样一系列分布<span class="math inline">\(G_1,G_2,\cdots\)</span>，对应离散数据上的离散分布，随机分布(测度)可以看成在多个点(atom)上的权重</p>
<p>采样自H得到<span class="math inline">\(\theta_1\)</span>(atom)，对应的权重<span class="math inline">\(\pi_1\)</span>由如下方式生成</p>
<ol type="1">
<li>采样自<span class="math inline">\(\Beta\)</span>分布得到<span class="math inline">\(\beta_1|\Beta(1,\alpha)\)</span></li>
<li><span class="math inline">\(\pi_1 =\beta_1\)</span></li>
</ol>
<blockquote>
<p><span class="math display">\[
E[\beta ]= \frac{1}{1+\alpha}
\]</span></p>
</blockquote>
<p>采样第二个atom <span class="math inline">\(\theta_2|H\)</span>，确定它的权重<span class="math inline">\(\pi_2\)</span></p>
<ol type="1">
<li><span class="math inline">\(\beta_2|\Beta(1,\alpha)\)</span></li>
<li><span class="math inline">\(\pi_2 = (1-\pi_1)\beta_2\)</span></li>
</ol>
<p>得到一系列样本和权重 <span class="math display">\[
\theta_1,\theta_2,\theta_3,\cdots\\
\pi_1,\pi_2,\pi_3,\cdots
\]</span> 对应分布G取值和概率，显然G是离散分布(无论H是否为离散分布)</p>
<ol type="1">
<li><span class="math inline">\(\alpha = 0,\beta=1\)</span>，第一次采样将所有权重给第一个样本，随后样本权重为0，生成单点分布</li>
<li><span class="math inline">\(\alpha = \infty,\beta\to 0\)</span>，每次采样生成的<span class="math inline">\(\beta \to 0,\pi\to 0\)</span>，权重都很小，分布退化为均匀分布</li>
</ol>
<h2 id="dirichlet-dsitribution">Dirichlet Dsitribution</h2>
<h3 id="beta分布">Beta分布</h3>
<p>Beya分布来自于顺序统计量，考虑从[0,1]均匀分布中采样N个数，第k大的数应该满足分布 <span class="math display">\[
f(x) =\frac{n!}{(k-1)!(n-k)!} x^{k-1} (1-x)^{n-k}=\frac{\Gamma(n+1)}{\Gamma(k)\Gamma(n-k+1)} x^{k-1} (1-x)^{n-k}
\]</span></p>
<blockquote>
<p>可以看成某种程度上的二项分布，落在左侧有k-1个数，右侧是n-k个数</p>
</blockquote>
<p><code>Gamma</code>函数将阶乘推广到实数域，令<span class="math inline">\(\alpha = k,\beta = n-k+1\)</span>，得到 <span class="math display">\[
f(x) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1} (1-x)^{\beta -1} = \frac{1}{\Beta(\alpha ,\beta)}x^{\alpha-1}(1-x)^{\beta -1} \\
\Gamma(Z) =\int_0^{\infty} t^{z-1} e^{-t} d t,\Gamma(n+1) = n!\\
\Beta(x,y) =\int_0^1 t^{x-1}(1-t)^{y-1} dt =\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}
\]</span></p>
<h3 id="从贝叶斯推断角度理解beta分布">从贝叶斯推断角度理解Beta分布</h3>
<p>Beta分布包含两个参数<span class="math inline">\(\Beta(\alpha,\beta)\)</span>，密度函数写成 <span class="math display">\[
f(x) =\frac{1}{\Beta(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta -1}
\]</span></p>
<ol type="1">
<li><span class="math inline">\(\alpha = \beta =1\)</span>，退化为均匀分布</li>
<li><span class="math inline">\(\alpha = \beta = k\neq 1\)</span>，为中心在<span class="math inline">\(\frac{1}{2}\)</span>的均匀分布</li>
</ol>
<p>抛硬币为例，假设抛了三次硬币都是正面，能否判断硬币的两面都是正面？贝叶斯分布用于刻画先验分布，即没有任何实验的前提下，事件：硬币两面都是正面的概率满足的分布</p>
<p>两点理由使用beta分布做先验分布</p>
<ol type="1">
<li><span class="math inline">\(\Beta\)</span>分布调整参数可以有效刻画多种不同分布</li>
<li><span class="math inline">\(\Beta\)</span>分布在二项实验结果产生后仍然为<span class="math inline">\(\Beta\)</span>分布</li>
</ol>
<p>假设初始假设硬币是均匀的，即<span class="math inline">\(\Beta(1,1)\)</span>，二项分布下<span class="math inline">\(X|p\)</span>满足二项分布(p是一次实验成功的概率) <span class="math display">\[
X|p \ bin(n,p)
\]</span> 后验分布<span class="math inline">\(f(p|X=k)\)</span>写成 <span class="math display">\[
f(p|X=k) =\frac{P(X=k|p)f(p)}{P(X=k)}
\]</span> 记作<span class="math inline">\(f(p|X=k)\)</span>正比于<span class="math inline">\(P(X=k|p)f(p)\)</span>，带入得到 <span class="math display">\[
\begin{aligned}
f(p|X=k) &amp;= P(X=k|p)\frac{1}{\Beta(\alpha,\beta)}p^{\alpha-1}(1-p)^{\beta-1}\\
&amp;=C_{n}^k p^{k+\alpha-1} (1-p)^{n-k+\beta-1} \frac{1}{\Beta(\alpha,\beta)}\
\end{aligned}
\]</span> 显然变成了<span class="math inline">\(\beta(k+\alpha,n-k+\beta)\)</span>分布</p>
<h3 id="beta-binomial共轭">Beta-Binomial共轭</h3>
<p>在n个数第k个数顺序统计量的前提下，增加一个限制。从<code>Uni(0,1)</code>中再采样m个数 <span class="math display">\[
Y_1,Y_2,\cdots,Y_m |Uni(0,1)
\]</span> 知道<span class="math inline">\(Y_i\)</span>中有<span class="math inline">\(m_1\)</span>个数比顺序统计量k<span class="math inline">\(x_{(k)}\)</span>大，<span class="math inline">\(m_2\)</span>个小，求 <span class="math display">\[
P(x_{(k)}|Y_1,Y_2,\cdots,Y_m)
\]</span> 分布</p>
<p>知道<span class="math inline">\(x_{(k)}\)</span>先验分布是<span class="math inline">\(\Beta\)</span>分布 <span class="math display">\[
f(x_{(k)}=x)|\Beta (k,n-k+1,x)
\]</span> <span class="math inline">\(Y_i\)</span>相当于做了m次伯努利实验，符合二项分布<span class="math inline">\(m_1|B(m,p)\)</span> <span class="math display">\[
P(m_1|x_{(k)}=x) =B(m,p)
\]</span> 后验分布写成 <span class="math display">\[
P(x_{(k)}= x|m_1) = \frac{P(m_1|x_{(k)}=x)P(x_{(k)}=x ) }{P(m_1)}
\]</span></p>
<blockquote>
<ol type="1">
<li>知识：随机采样m个数，包含<span class="math inline">\(m_1\)</span>个数比X小，其余数比X大</li>
<li>先验：X采样自[0,1]上的分布<span class="math inline">\(f(x)\)</span></li>
</ol>
<p>知识表示为 <span class="math display">\[
P(m=m_1|x) = \frac{m!}{m_1!m_2!} x^{m_1}(1-x)^{m_2}  \\
\begin{aligned}
P(m=m_1)&amp; = \int_0^1 P(m=m_1|x) f(x) dx \\
&amp;=\int_0^1 \frac{m+1}{\Beta(m_1+1,m_2+1)} x^{m_1+k-1} (1-x)^{m_2+n-k} \frac{1}{\Beta(k,n-k+1)}\\
&amp;=(m+1) \frac{1}{\Beta(m_1+1,m_2+1)\Beta(k,n-k+1)}\Beta(m_1+k,m_2+n-k+1)
\end{aligned}
\]</span> 后验分布计算为 <span class="math display">\[
\begin{aligned}
f(x|m_1) &amp;= \frac{p(m_1|x)p(x)}{p(m_1)}\\
\end{aligned}
f(x|m_1)|\Beta(x|k+m_1,n-k+1+m_2)
\]</span></p>
</blockquote>
<h3 id="dirichlet-multinomial共轭">Dirichlet-Multinomial共轭</h3>
<p>考虑排序后的顺序统计量 <span class="math display">\[
X_1,X_2,\cdots,X_n
\]</span> 求 <span class="math display">\[
X_{k_1},X_{k_1+k_2}
\]</span> 的联合分布</p>
<p>从另一个角度考虑这个问题，<span class="math inline">\(X_{k_1},X_{k_1+k_2}\)</span>将区间分成三个部分</p>
<ol type="1">
<li><span class="math inline">\([0,x_1)\)</span>包含<span class="math inline">\(k_1-1\)</span>个元素</li>
<li><span class="math inline">\((x_1,x_1+x_2)\)</span>包含<span class="math inline">\(k_2-1\)</span>个元素</li>
<li><span class="math inline">\((x_1+x_2,1]\)</span>包含<span class="math inline">\(N - k_1 -k_2\)</span>个元素</li>
</ol>
<blockquote>
<p>这显然是一个多项式分布：假设做一次随机试验A存在三种结果，概率分别为 <span class="math display">\[
P(A_1) = p_1\\
P(A_2) = p_2\\
P(A_3) = p_3,p_1+p_2+p_3=1
\]</span> 做N次A实验，求 <span class="math display">\[
P(A_1 = k_1,A_2=k_2,A_3 = k_3),k_1+k_2+k_3 = N
\]</span> 概率 <span class="math display">\[
P(A_1= k_1,A_2 = k_2,A_3 = k_3) =C_{N}^{k_1,k_2,k_3} p_1^{k_1} p_2^{k_2}p_3^{k_3}
\]</span> 可以理解为 <span class="math display">\[
\begin{aligned}
P(A_1=k_1,A_2=k_2,A_3=k_3) &amp;= P(A_1=k_1,A_2=k_2|A_3=k_3)P(A_3=k_3)\\
&amp;=C_{N-k_3}^{k_1}  (\frac{p_1}{p_1+p_2})^{k_1}(\frac{p_2}{p_1+p_2})^{k_2} C_{N}^{k_3} p_3^{k_3} (p_1+p_2)^{k_1+k_2}\\
=\frac{N!}{k_3!k_1!k_2!} p_1^{k_1} p_2^{k_2} p_3^{k_3}
\end{aligned}
\]</span> 记作<span class="math inline">\(Multi(N,p_1,p_2,p_3)\)</span>，意味着做N次实验，每个事件发生的概率分别为<span class="math inline">\(p_1,p_2,p_3\)</span></p>
</blockquote>
<p>求解(先选择2个元素放在<span class="math inline">\(x_1,x_2\)</span>，随后变成<span class="math inline">\(Multi(N-2,x_1,x_2,x_3=(1-x_1-x_2))\)</span>) <span class="math display">\[
\begin{aligned}
f(x_1,x_2,x_3) &amp;= A_n^2 Multi(n-2,x_1,x_2,x_3)\\
&amp;= n(n-1)\frac{(n-2)!}{(k_1-1)!(k_2-1)!(n-k_1-k_2)!} x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}\\
&amp;= \frac{\Gamma(n+1)}{\Gamma(k_1)\Gamma(k_2)\Gamma(n-k_1-k_2+1)}
\end{aligned}
\]</span> 记作Dirichlet分布，同理可以讨论<span class="math inline">\(x_{k_1},x_{k_1+k_2},x_{k_1+k_2+k_3},\cdots\)</span>的联合分布，Dirichlet分布<span class="math inline">\(Dir(k_1,K_2,k_3)\)</span>写成 <span class="math display">\[
f(x_1,x_2,x_3) = \frac{\Gamma (\alpha_1+\alpha_2+\alpha_3)}{\Gamma (\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}x_1^{\alpha_1-1}x_2^{\alpha_2-1}x_3^{\alpha_3-1}\\
\alpha_1 = k_1,\alpha_2=k_2,\alpha_3 = n-k_1-k_2+1,x_1+x_2+x_3=1
\]</span></p>
<h3 id="数学期望和方差">数学期望和方差</h3>
<blockquote>
<p>数学期望可以从如下角度考虑，对于<span class="math inline">\(x_1\)</span>的期望，实际上是顺序统计量<span class="math inline">\(x_{k_1}\)</span>的期望，计算 <span class="math display">\[
f(x_{k_1} = x) = C_{n}^{k_1}  x^{k_1}(1-x)^{n-k_1}\\
\begin{aligned}
E[x_1] &amp;= \frac{n!}{(n-k_1)!k_1!}\int_0^1 x^{k_1+1}(1-x)^{n-k_1} dx\\
&amp;=\frac{\Gamma(n+1)}{\Gamma(n-k_1+1)\Gamma(k_1+1)}\Beta(k_1+2,n-k_1+1)\\
&amp;=\frac{\Gamma(n+1)\Gamma(k_1+2)}{\Gamma(k_1+1)\Gamma(n+3)}\\
&amp;=\frac{k_1+1}{(n+2)(n+1)},n=\alpha_1+\alpha_2+\alpha_3-1
\end{aligned}
\]</span></p>
</blockquote>
<h2 id="characteristic-of-dirichlet-process">Characteristic of Dirichlet Process</h2>
<p>牢记：随机过程采样得到随机分布</p>
<p>Dirichlet Process任何时刻得到的分布G是离散分布，在离散点<span class="math inline">\(a_1,a_2,\cdots,a_d\)</span>上联合分布满足 <span class="math display">\[
(G(a_1),G(a_2),\cdots,G(a_d))|Dir(\alpha H(a_1),\alpha H(a_2),\cdots,\alpha H(a_d))
\]</span></p>
<blockquote>
<p><span class="math display">\[
x_1,x_2,\cdots,x_n|Dir(\alpha_1,\alpha_2,\cdots,\alpha_d)\\
E[x_i] = \frac{\alpha_i}{\sum_j \alpha_j}\\
Var[x_i]=\frac{\alpha_i(\sum_k \alpha_k - \alpha_i)}{(\sum_k \alpha_i)^2(\sum_k \alpha _k +1)}
\]</span></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/17/Dirichletprocess1/" data-id="cleze2otv000osg8wdxjhcv6f" data-title="Dirichletprocess1" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Random-Process-Dirichlet-Process/" rel="tag">Random Process, Dirichlet Process</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ContrastiveLearningOverview3" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/17/ContrastiveLearningOverview3/" class="article-date">
  <time class="dt-published" datetime="2023-02-17T11:28:13.000Z" itemprop="datePublished">2023-02-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/17/ContrastiveLearningOverview3/">ContrastiveLearningOverview3</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="contrastive-learning-introductioniii不用负样本的对比学习">Contrastive Learning Introduction(III，不用负样本的对比学习)</h1>
<h2 id="byolbootstrap-your-own-latent-a-new-approach-to-self-supervised-learning">BYOL(Bootstrap Your Own Latent A New Approach to Self-Supervised Learning)</h2>
<p>通过自举(仅仅在正样本之间对比学习)学习好的特征表示，同时避免模型退化</p>
<figure>
<img src="https://s2.loli.net/2023/02/16/mJBUMe4r8oCGj9A.png" alt="image-20230216175143509" /><figcaption aria-hidden="true">image-20230216175143509</figcaption>
</figure>
<p>同样的instance通过两个数据增强得到<span class="math inline">\(v\)</span>和<span class="math inline">\(v^\prime\)</span>，通过两个编码器<span class="math inline">\(f_\theta,f_\xi\)</span>和两个projection head <span class="math inline">\(g_\theta,g_\xi\)</span>得到representation。使用动量更新<span class="math inline">\(f_\xi,g_\xi\)</span></p>
<h3 id="predictor">Predictor</h3>
<p>增加一个Prediction Network <span class="math inline">\(q_\theta\)</span>(MLP)，使得预测结果和<span class="math inline">\(z_\xi^\prime\)</span>尽量接近，输出的<span class="math inline">\(y_\theta\)</span>用于下游任务</p>
<h3 id="loss">Loss</h3>
<p><span class="math display">\[
\parallel q_\theta(z_\theta) - z^\prime_\xi\parallel_2^2
\]</span></p>
<h3 id="问什么不会出现模型坍塌">问什么不会出现模型坍塌</h3>
<p>projector/prediction包含两个Batch Normal <span class="math display">\[
Linear \to BN \to ReLU\to BN\tag{MLP}\label{MLP}
\]</span> Batch Normal会泄露样本中其它数据的特征(和平均数据的差别)，本质上是隐式的对比试验</p>
<h2 id="simsiamexploring-simple-siamese-representation-learning">SimSiam(Exploring Simple Siamese Representation Learning)</h2>
<ol type="1">
<li>无需正样本</li>
<li>无需batch size</li>
<li>无需动量编码器</li>
</ol>
<figure>
<img src="https://s2.loli.net/2023/02/16/lJQzLWFV9gHeid6.png" alt="image-20230216180732377" /><figcaption aria-hidden="true">image-20230216180732377</figcaption>
</figure>
<p>共享参数的两个编码器(没有动量更新，完全copy参数)，predictor同时作用于<span class="math inline">\(x_1,x_2\)</span>，随后互相预测，计算<code>mse loss</code> <span class="math display">\[
x_1 \to p_1\to h_1,x_2\to p_2\to h_2\\
\mathcal{L} = D(p_1,h_2)+D(p_2,h_1)
\]</span></p>
<h3 id="几个工作的对比">几个工作的对比</h3>
<figure>
<img src="https://s2.loli.net/2023/02/16/jK8zE5PvwMO4XHt.png" alt="image-20230216181120466" /><figcaption aria-hidden="true">image-20230216181120466</figcaption>
</figure>
<ol type="1">
<li><code>SimCLR</code>和<code>SwAV</code>计算正负样本embedding，优化<code>infonce loss</code>，区别是前者需要迭代两个编码器，后者只需要更新一个</li>
<li><code>BYOL</code>和<code>SimSiam</code>无需计算<code>infonce loss</code>，转化为预测问题，前者采用动量更新保持参数一致性，后者直接采用同一个网络+stop gradient</li>
</ol>
<h2 id="结合transformer和对比学习">结合Transformer和对比学习</h2>
<h3 id="moco-v3">Moco-v3</h3>
<p>解决自监督Transformer训练不稳定的问题，Backbone换成visual transformer，这种情况下大batch_size performance反而不好</p>
<blockquote>
<p>随机初始化patch transformer(tokenization，图片序列化)</p>
</blockquote>
<p>结合<code>infonce loss</code>和<code>predictive loss</code></p>
<h3 id="dinoself-distillation-with-no-labels">Dino(Self-distillation with no labels)</h3>
<figure>
<img src="https://s2.loli.net/2023/02/16/KUqm7XgtpcTxOu8.png" alt="image-20230216200246056" /><figcaption aria-hidden="true">image-20230216200246056</figcaption>
</figure>
<p>添加centering避免模型坍塌</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/17/ContrastiveLearningOverview3/" data-id="cleze2otp000esg8w1lyh0kcq" data-title="ContrastiveLearningOverview3" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Contrastive-Learning-Deep-Learning-Unsupervised-Learning/" rel="tag">Contrastive Learning, Deep Learning, Unsupervised Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ContrastiveLearningOverview1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/17/ContrastiveLearningOverview1/" class="article-date">
  <time class="dt-published" datetime="2023-02-17T11:28:09.000Z" itemprop="datePublished">2023-02-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/17/ContrastiveLearningOverview1/">ContrastiveLearningOverview1</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="paper-readingmomentum-contrast-for-unsupervised-visual-representation-learning">Paper Reading(Momentum Contrast for Unsupervised Visual Representation Learning)</h1>
<p>无监督学习预训练的结果逼近有监督学习，对比学习用来挖掘instance之间的相似程度</p>
<h2 id="背景">背景</h2>
<p>相当于给样本打上伪标签(相似/不相似)</p>
<h3 id="instance-discrimination">Instance Discrimination</h3>
<p>给定一个数据集 <span class="math display">\[
x_1,x_2,\cdots,x_n
\]</span> 对每个instance用k种transformer生成k个增广instance <span class="math display">\[
x_1^k ,x_1^2,\cdots,x_1^k
\]</span> 在<span class="math inline">\(n\times (k+1)\)</span>大小的数据集中，instance和它的增广互为正样本(相似)，其余样本互为负样本</p>
<blockquote>
<p>每张图片自成一类</p>
</blockquote>
<p>学习特征，带入NCE loss</p>
<h3 id="动量">动量</h3>
<p>加权平均 <span class="math display">\[
y_t = my_{t-1}+(1-m)x_t
\]</span> 当前输入依赖于上一时刻状态</p>
<h2 id="abstract-introduction">Abstract &amp;&amp; Introduction</h2>
<p><span class="math display">\[
对比学习\to 字典查询\\
memory\ bank\to 队列
\]</span></p>
<p>moving average encoder用于提取特征，采用动量更新的方法尽量和上一时刻状态保持一致</p>
<blockquote>
<p>Moco学习的特征可以简单地迁移到下游任务</p>
</blockquote>
<h3 id="linear-protocol">Linear Protocol</h3>
<p>测试特征提取网络的好坏，仅仅训练全连接网络作为分类器并在梯度更新时freeze特征提取网络</p>
<h3 id="gap-between-supervised-and-unsupervised-learning">gap between supervised and unsupervised Learning</h3>
<p>对比学习=构建动态字典</p>
<p>对于<span class="math inline">\(x_1\)</span>增广得到正样本对<span class="math inline">\(x_1^1,x_1^2\)</span>，相对于数据集中其它样本<span class="math inline">\(x_2,x_3,\cdots,x_n\)</span>是负样本 <span class="math display">\[
x_1^1\to^{E_1}\to f_1^1\\
x_1^2 \to ^{E_2}\to f_1^2\\
x_2,x_3,\cdots,x_n\to ^{E_2}\to f_n
\]</span> 两个特征提取器获取特征(也可以借助单一net)，字典查询指的是从一些key中找到和query相似的key，这里</p>
<ol type="1">
<li>query：<span class="math inline">\(f_1^1\)</span>正样本编码出的特征</li>
<li>keys：<span class="math inline">\(f_1^2,f_2,f_3,\cdots,f_n\)</span>，负样本编码出的特征，希望查找到<span class="math inline">\(f_1^2\)</span></li>
</ol>
<p>重新记作 <span class="math display">\[
x_q\to^{E_1}\to q\\
x_k\to^{E_2}\to k_0,k_1,\cdots
\]</span> 希望</p>
<ol type="1">
<li>字典尽量大</li>
<li>编码器一致性，指的是多个key应该借助相似的编码器生成</li>
</ol>
<h3 id="为什么用队列代替字典">为什么用队列代替字典？</h3>
<p>队列可以很大，但是每次更新只更新一个mini-batch大小的特征</p>
<blockquote>
<p>这样会破坏队列中元素的一致性，具体而言队列头和队列尾部特征来自不同的编码器，头的特征来自<strong>旧编码器</strong>，尾部特征来自<strong>新编码器</strong></p>
</blockquote>
<p>momentum encoder解决不一致问题，它的更新满足 <span class="math display">\[
\theta_k = m \theta_{k-1}+(1-m)\theta_q
\]</span> 落后于正样本encoder参数</p>
<blockquote>
<p>能否将Mask Auto Encoding作为代理任务？</p>
</blockquote>
<h2 id="论文精读">论文精读</h2>
<ol type="1">
<li>intro部分归纳无监督预训练在CV上表现不佳的原因在于离散/连续信号空间带来的差别，NLP上可以建立关于token的字典</li>
<li>Dynamic Dictionary: key采样自data，通过encoder编码为embedding，问题转化为字典中query查找的过程</li>
</ol>
<figure>
<img src="https://s2.loli.net/2023/02/01/JMhfc15pO48Lb2i.png" alt="image-20230201114411771" /><figcaption aria-hidden="true">image-20230201114411771</figcaption>
</figure>
<ol start="3" type="1">
<li>字典被视为一个队列，当前mini-batch encode结果入队，旧encoder生成的结果出队。动量更新保证队列中元素的一致性</li>
<li>can be transferred to downstream tasks by fine-tuning</li>
</ol>
<h3 id="method-contrast-learning-as-dictionary-look-up">Method: Contrast Learning as Dictionary Look-up</h3>
<p>给定query q和一系列keys <span class="math inline">\(\{ k_0,k_1,\cdots,\}\)</span>，其中包含唯一key <span class="math inline">\(k_+\)</span>是查询的目标，Contrast Loss用于评估q和<span class="math inline">\(k_+\)</span>相对于其它正负样本的相似程度，查询记作 <span class="math display">\[
q=f_q(x^q)
\]</span></p>
<ol type="1">
<li><span class="math inline">\(f_q\)</span> encoder network</li>
<li><span class="math inline">\(x^q\)</span> query sample</li>
</ol>
<h3 id="contrast-loss梯度回传的方式-momentum-contrast">Contrast Loss梯度回传的方式 Momentum Contrast</h3>
<figure>
<img src="https://s2.loli.net/2023/02/01/hwu1PjkBW5EgATo.png" alt="image-20230201152208102" /><figcaption aria-hidden="true">image-20230201152208102</figcaption>
</figure>
<ol type="1">
<li>end-to-end 通过contrastive loss同时更新key encoder和query encoder的参数，两个encoder不共享任何参数</li>
<li>memory-bank 使用一个encoder，维护memory bank，存储所有正负样本的embedding tensor(从梯度图上detach)。contrastive loss回传每更新一次encoder就重新计算一次memory bank。每次从memory bank中采样若干negative samples</li>
<li><code>Moco</code>有些类似强化学习中延迟更新的思想，key encoder采用动量更新</li>
</ol>
<h4 id="use-queue-instead-of-memory-bank">Use queue instead of memory bank</h4>
<p>队列保证我们可以使用之前encoder的结果，希望key encoder改变不要太大因此使用动量更新的策略</p>
<blockquote>
<p>相比memory bank，避免了每次更新encoder都需要重新遍历data sample带来的开销</p>
</blockquote>
<h3 id="pretext-task构建正负样本对">Pretext Task构建正负样本对</h3>
<p>构建正样本对：同一张图片采取两种方式进行数据增强</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/17/ContrastiveLearningOverview1/" data-id="cleze2otj0004sg8w0hds5ble" data-title="ContrastiveLearningOverview1" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Contrastive-Learning-Deep-Learning-Unsupervised-Learning/" rel="tag">Contrastive Learning, Deep Learning, Unsupervised Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ContrastiveLearningOverview2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/14/ContrastiveLearningOverview2/" class="article-date">
  <time class="dt-published" datetime="2023-02-14T04:15:07.000Z" itemprop="datePublished">2023-02-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/14/ContrastiveLearningOverview2/">ContrastiveLearningOverview2</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="对比无监督学习综述ii">对比无监督学习综述(II)</h1>
<h2 id="moco">Moco</h2>
<h3 id="contribution">Contribution</h3>
<ol type="1">
<li>对比学习$$字典查询</li>
<li>队列+动量编码器解决
<ol type="1">
<li>字典信息不一致</li>
<li>每次更新参数需要重新计算所有key embedding导致较大计算开销(而不是动量更新特征)</li>
</ol></li>
</ol>
<h3 id="实现">实现</h3>
<ol type="1">
<li>数据增强，随机采样图片的子模块</li>
<li><span class="math inline">\(\tau = 0.07\)</span></li>
</ol>
<h3 id="moco-v2">Moco-v2</h3>
<p>在Moco基础上结合SimCLR提升Moco</p>
<ol type="1">
<li>加了MLP</li>
<li>数据增强</li>
<li>更多epoch</li>
</ol>
<h2 id="simclrsimple-contrastive-learning">SimCLR(Simple Contrastive Learning)</h2>
<h3 id="methods">Methods</h3>
<p>对Mini-batch中的所有图片<span class="math inline">\(x\)</span>，数据增强得到 <span class="math display">\[
\hat x_i,\hat x_j
\]</span> 互为正样本，和batch中之外的图片增强的样本互为负样本，通过共享权重的编码器得到表示 <span class="math display">\[
h_i = f(\hat x_i),h_j = f(\hat x_j)
\]</span> 引入一个mlp层 <span class="math inline">\(g(\cdot)\)</span> <span class="math display">\[
z_i = g(h_i),z_j = g(h_j)
\]</span> 在<span class="math inline">\(z_i,z_j\)</span>之间做对比学习，采用normalized temperature-scaled交叉熵函数</p>
<ol type="1">
<li>归一化特征</li>
<li>temperature作为loss函数的权重</li>
</ol>
<p>mlp层<span class="math inline">\(g(\cdot)\)</span>仅仅用于训练，下游任务中只用h特征(预训练)。本文另一个特点是无需计算所有正负样本，所有正负样本对来自同一个mini-batch</p>
<h3 id="前向工作invaspread">前向工作：InvaSpread</h3>
<p>mini-batch中相似的图片应该得到相似的特征表示(数据增强得到正样本)</p>
<blockquote>
<p>和SimCLR相比batch_size选择的太小，导致负样本不是特别多(更多的数据增强，更大的batch_size)</p>
</blockquote>
<h3 id="experiment">Experiment</h3>
<ol type="1">
<li>随机的裁剪和颜色变换是最有用的image trans</li>
<li>非线性编码器<span class="math inline">\(g(\cdot)\)</span>比仅仅加一层MLP存在性能提升，但是特征维度并没有太大的影响</li>
</ol>
<h3 id="simclrv2半监督学习">SimCLRv2（半监督学习）</h3>
<ol type="1">
<li>大量无监督数据对比学习方法预训练</li>
<li>少量有标签数据微调</li>
<li>生成teacher模型，为无监督数据打标签</li>
</ol>
<p>相比SimCLR</p>
<ol type="1">
<li>更大的模型(backbone)</li>
<li>更深的非线性函数<span class="math inline">\(g(\cdot)\)</span></li>
<li>动量编码器</li>
</ol>
<h2 id="swavunsupervised-learning-of-visual-features-by-contrasting-clustering-assignment">SwAV(Unsupervised Learning of Visual Features by Contrasting Clustering Assignment)</h2>
<h3 id="motivation">motivation</h3>
<p>同一场景下对应不同view，给定一个view下的特征，能否推断另一view下的特征。</p>
<h3 id="compared-with-contrastive-learning">compared with Contrastive Learning</h3>
<figure>
<img src="https://s2.loli.net/2023/02/14/H4xqmeBMDCNdSWQ.png" alt="image-20230214115041760" /><figcaption aria-hidden="true">image-20230214115041760</figcaption>
</figure>
<p>instance classification这个任务要求自己和自己比，这个工作要求每个样本和<strong>聚类中心</strong>对比，<span class="math inline">\(c\in \R^{D\times K}\)</span></p>
<ol type="1">
<li>K c dimension of center embedding</li>
<li>D number of centers</li>
</ol>
<p>生成同一样本两种数据增强的提取的特征 <span class="math display">\[
x\to z_1,z_2
\]</span> 基于center生成两个目标哦 <span class="math display">\[
c\times z_1\to Q_1,c\times z_2\to Q_2
\]</span> 如果<span class="math inline">\(z_1,z_2\)</span>相似，可以借助<span class="math inline">\(z_1\)</span>和c预测<span class="math inline">\(Q_2\)</span></p>
<p><span class="math inline">\(Q\)</span>在某种程度上是latent embedding z的预测结果的ground truth，通过swap prediction进行训练</p>
<h3 id="advantage">Advantage</h3>
<ol type="1">
<li>减少负样本的依赖</li>
<li>聚类中心具有特定语义含义(代表某一类数据)，传统Contrastive Learning中不能区分不同负样本和正样本的相似程度(类别不均衡)</li>
</ol>
<h3 id="multi-crop">Multi-Crop</h3>
<p>生成来自一个图片的两个正样本，往往正样本之间有重叠，希望同时使用多个正样本同时不增加计算成本</p>
<ol type="1">
<li>选择两个较大的crop学习全局特征</li>
<li>选择若干较小crop学习局部特征</li>
</ol>
<blockquote>
<p>Multi-view</p>
</blockquote>
<h3 id="refer">Refer</h3>
<p>Deep clustering</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/14/ContrastiveLearningOverview2/" data-id="cleze2otl0008sg8w3lzw7nzm" data-title="ContrastiveLearningOverview2" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Contrastive-Learning-Deep-Learning-Unsupervised-Learning/" rel="tag">Contrastive Learning, Deep Learning, Unsupervised Learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-firstpost" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/13/firstpost/" class="article-date">
  <time class="dt-published" datetime="2023-02-13T12:43:15.000Z" itemprop="datePublished">2023-02-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/13/firstpost/">How to post a new article and establish local site with hexo</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="use-of-hexo">Use of Hexo</h1>
<p>Thanks to Chatgpt which points out the right way to use Hexo to establish a local site and pub it to github service. Here are the record to publish a new article into github ## make a new article 1. enter hexo site dir 2. create a new article <code>hexo new articletitle</code> articletitle is the title of your article. Then you will find a new markdown file named <code>articletitle.md</code> in the <code>source/_post</code> dir 3. edit the markdown file and add tags to it</p>
<h2 id="post-local-dir-to-github-page-service">post local dir to github page service</h2>
<p>When you encounter a newly created hexo repo(without relationship with any github pages) 1. create repo in remote github service 2. config the _config.yml file deploy sector. 1. type:git 2. repo:Your github.io repo 3. branch:master 3. install git deploy extension with <code>npm install hexo-deployer-git --save</code>(This may fail because of file lock,for example you have opened the github repo locally in a text editor) 4. generate static pages with <code>hexo generate</code> 5. deploy into pages <code>hexo deploy</code> 6. you will see your pages later</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/13/firstpost/" data-id="cleze2otk0005sg8wczgi12fl" data-title="How to post a new article and establish local site with hexo" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Use-of-Hexo/" rel="tag">Use of Hexo</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/13/hello-world/" class="article-date">
  <time class="dt-published" datetime="2023-02-13T12:24:14.062Z" itemprop="datePublished">2023-02-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/13/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/13/hello-world/" data-id="cleze2oto000csg8w372j2s9e" data-title="Hello World" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm-design-and-Analysis-Approximation-Algorithm/" rel="tag">Algorithm design and Analysis, Approximation Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Contrastive-Learning-Deep-Learning-Unsupervised-Learning/" rel="tag">Contrastive Learning, Deep Learning, Unsupervised Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Random-Process-Dirichlet-Process/" rel="tag">Random Process, Dirichlet Process</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Use-of-Hexo/" rel="tag">Use of Hexo</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm-design-and-Analysis-Approximation-Algorithm/" style="font-size: 20px;">Algorithm design and Analysis, Approximation Algorithm</a> <a href="/tags/Contrastive-Learning-Deep-Learning-Unsupervised-Learning/" style="font-size: 15px;">Contrastive Learning, Deep Learning, Unsupervised Learning</a> <a href="/tags/Random-Process-Dirichlet-Process/" style="font-size: 10px;">Random Process, Dirichlet Process</a> <a href="/tags/Use-of-Hexo/" style="font-size: 10px;">Use of Hexo</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/02/19/Approximation-Algorithm-Sparest-Cut/">Approximation Algorithm Sparest Cut</a>
          </li>
        
          <li>
            <a href="/2023/02/19/Approximation-Algorithm-TSP-Max-2-SAT/">Approximation Algorithm TSP Max 2-SAT</a>
          </li>
        
          <li>
            <a href="/2023/02/19/Approximation-Algorithm-Maxcut-and-Correlation-Cluster/">Approximation Algorithm Maxcut and Correlation Cluster</a>
          </li>
        
          <li>
            <a href="/2023/02/19/hardness-of-approximation/">hardness of approximation</a>
          </li>
        
          <li>
            <a href="/2023/02/17/Dirichletprocess1/">Dirichletprocess1</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Haihan Gao<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>